---
title: "New York Times Analysis"
author: "Daniel Wang, Pratiksha Sharma"
date: '2022-05-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(tidyverse)
library(httr)
library(curl)
library(jsonlite)
library(rvest)
library(tm)
library(tidytext)
library(topicmodels)
library(textdata)
```

Welcome! When we first started this project, all we knew is that we wanted to take advantage of the New York Times API to do some kind of text mining analysis on various articles. After doing some initial analysis, we decided that we wanted to take you on a journey exploring what New York Times has to say about the sensational celebrity court case that has taken the nation by storm, Johnny Depp vs Amber Heard. By conducting LDA Analysis and Sentiment Analysis, we investigate and compare various aspects of the articles and reviews surrounding these two individuals.

However, before we get started, let's first delve into our initial analyses and investigations into the New York Times API. Now, the NYT API is not a simple one stop shop with all of the data we need neatly put together. It is actually a collection of different API's which have different purposes. The first one we decided to investigate was the Most Popular API.

First, we extracted the data from the Most Popular using a url with our free API Key which looks like:

https://api.nytimes.com/svc/mostpopular/v2/viewed/1.json?api-key=YOUR-API-KEY-HERE

We collected the data and used R's jsonlite to read it into a data frame. Note that due to the nature of the API, we are only able to obtain 20 articles.
```{r echo=FALSE}
url2 <- 'https://api.nytimes.com/svc/mostpopular/v2/viewed/1.json?api-key=52utY7TeqStbbVwadabbLam6qUdac7d5'
json_result2 <- url2 %>% curl() %>% readLines() %>% fromJSON()
```

After obtaining the data, we discovered that it did not actually contain the contents of the most popular articles, but rather just their html links. Therefore, we had to create a function to extract the text out of these articles and compile them into one large data frame. This function uses HTML web scraping techniques to obtain the text of the article body.

```{r}
extract_text_mp <- function(json_result){
  articles <- json_result$results %>% 
    dplyr::select(c('title','url'))
  article_text <-tibble(headline = '',text='')
  for(i in 1:nrow(articles)){
    url_html <- articles[i,2]
    article_text <- article_text %>% add_row(headline = articles[i,1], text = url_html %>% read_html() %>% html_nodes("section") %>% html_nodes("p") %>% html_text2())
  }
  article_text <- article_text %>% group_by(headline) %>% summarise(text = paste(text,collapse = " "))
  article_text <- article_text[-c(1),] 
  return(article_text)
}
```

Then, we use this function to extract the text from the most popular articles. After obtaining the raw text, we have to clean it to prepare it for our text mining analyses. We do so by making all the text lower case, squashing contractions, removing punctuation, and possibly most importantly, removing stop words.

```{r}
mp_text <- extract_text_mp(json_result2)
mp_text <- mp_text %>% mutate(
  clean_text = tolower(text),
  clean_text = removeWords(clean_text,stop_words$word),
  clean_text = str_replace_all(clean_text,"[['`’$+]]", ""), 
  clean_text = str_replace_all(clean_text,"[[:punct:]]", " "),
  clean_text = str_replace_all(clean_text,'[[:digit:]]+', " "),
  clean_text = str_replace_all(clean_text,"[[:space:]]+", " "),
  clean_text = str_replace_all(clean_text," s ", " "),
  clean_text = trimws(clean_text)) 
```

Now, after doing all this data cleaning and preparation, it's time to do our analysis. Let's start off with Latent Dirichlet allocation, or LDA. This is basic Topic Modelling, and the function we are using requires our data be in a Document Term Matrix, so let's take care of that first.

```{r}
dtm_mp <- tm::DocumentTermMatrix(tm::VCorpus(tm::VectorSource(mp_text$clean_text)),control=list(wordLengths=c(1,Inf)))
dtm_mp
```

Now that our data is in the form of a DTM, we can easily run a LDA model on it. Let's split our articles into two topics.

```{r}
mp_lda <- LDA(dtm_mp, control = list(seed=0), k = 2)
mp_topics <- tidy(mp_lda)
```

Using this model, we have now separated our articles into 2 models. Let's take a look at the 10 most popular words in each topic.

Topic 1:
```{r echo = FALSE}
mp_topics %>% filter(topic==1) %>% 
  mutate(beta_rank = min_rank(desc(beta))) %>% 
  filter(beta_rank <= 10) %>% 
  arrange(beta_rank) %>%
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(beta, term)) + 
  geom_col(show.legend = FALSE)

```

Topic 2:
```{r echo = FALSE}
mp_topics %>% filter(topic==2) %>% 
  mutate(beta_rank = min_rank(desc(beta))) %>% 
  filter(beta_rank <= 10) %>% 
  arrange(beta_rank) %>%
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(beta, term)) + 
  geom_col(show.legend = FALSE)
```

(Note that these results are different than the ones in our presentation because our call to the API retrieves the most popular articles in the last day)

Our second topic is likely centered around the domestic and foreign political issues, whereas for Topic 1, we have a clear legal theme. We can specifically pinpoint the hot button issues of the recent Supreme Court leak which suggests the overturning of Roe v. Wade, and of course, the Amber Heard and Johnny Depp case.

Since we have two topics, we can also do a further analysis to see which words are especially more prevalent in one topic but not the other. We do this for words that at least have a minimal probability in one of the two topics. To do this, we use a log comparison between the probability that a word appears in one topic vs the other.

```{r}
beta_wide <- mp_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > 0.001 | topic2 > 0.001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
```

These words are more prevalent in Topic 1.

```{r echo = FALSE}
beta_wide %>% arrange(log_ratio) %>%
  mutate(term = reorder(term, log_ratio)) %>% 
  head(10) %>%
  ggplot(aes(log_ratio, term)) + 
  geom_col(show.legend = FALSE)
```

These words are more prevalent in Topic 2.

```{r echo = FALSE}
beta_wide %>% arrange(desc(log_ratio)) %>%
  mutate(term = reorder(term, desc(log_ratio))) %>% 
  head(10) %>%
  ggplot(aes(log_ratio, term)) + 
  geom_col(show.legend = FALSE)
```

Using these graphs we can corroborate the fact that Topic 1 has a legal theme and Topic 2 has a political theme.

We also did sentiment analysis on these most popular articles.

This code unnests the article texts into words which facilitates sentiment analysis

```{r}
sa_text = mp_text %>% group_by(headline) %>% unnest_tokens(word, clean_text)
```

Now, we start by using NRC sentiment analysis on these most popular articles and plot these sentiments

```{r}
sa_processed_text = sa_text %>% inner_join(get_sentiments("nrc")) %>% group_by(headline) %>% count(sentiment, sort = TRUE)

# Now, we plot this:

sa_processed_text %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Sentiment") + ylab("Number of Words") + ggtitle("Sentiments in Most Popular Articles") + theme(legend.position="none")
```

The graph shows us that the articles had a majorly positive sentiment. 

But, as mentioned before, the apparent hot topic is that of the Johnny Depp v. Amber Heard Trial, so we pick up an article about that and look closely at it for specific sentiment analysis.

We generate plots that show NRC and Bing analysis on it:

```{r}
sa_text %>% filter(headline == "Johnny Depp v. Amber Heard: What We Know") %>% inner_join(get_sentiments("nrc")) %>% count(sentiment, sort = TRUE) %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Sentiment") + ylab("Number of Words") + ggtitle("Johnny Depp v. Amber Heard: NRC SA") + theme(legend.position="none")

sa_text %>% filter(headline == "Johnny Depp v. Amber Heard: What We Know") %>% inner_join(get_sentiments("bing")) %>% count(sentiment, sort = TRUE) %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Sentiment") + ylab("Number of Words") + ggtitle("Johnny Depp v. Amber Heard: Bing SA") + theme(legend.position="none")
```

The graphs above show that the overall topic has a very negative sentiment attached to it as seen through analysis of this article. Negative, fear and anger are leading emotions in nrc and negative is extremely dominant in the bing analysis as well.

It would be interesting to further look into Johnny Depp, Amber Heard, individually and their relationship. 

We have to switch gears to another New York Times API here, but before doing that, we perform one final Aggregate Afinnity Score Analysis on the most popular articles. 

But simply finding afinn scores for each word in the article does not make a lot of sense since the result does not portray an overall sentiment for a particular article. For NRC and Bing, you can filter results for a particular movie and the observations make sense, but there needs to be some aggregation required for afinn results to make sense.

Below, we aggregate and assign an affinity score to each article that are most popular and plot these scores:

```{r}
sa_text_agg = sa_text %>% inner_join(get_sentiments("afinn"))  
aggregate(x = sa_text_agg$value, by = list(sa_text_agg$headline),FUN = sum) %>% arrange(x) %>% ggplot(aes(seq(1:20), x)) + geom_bar(stat = "identity") + xlab("Dummy Article Number") + ylab("Afinnity Score") + ggtitle("Afinnity of Most Popular Articles") + theme(legend.position="none")
```

Note: Article numbers are dummy here

Moving on, after deciding to focus on the issue of Johnny Depp and Amber Heard, we first wanted to explore their professional lives by taking advantage of the NYT Movie Reviews API. So we queried the API for all movie reviews containing either Johnny Depp or Amber Heard and then performing various analyses to compare their work.

We omit the code for extracting and cleaning the data as it is nearly identical to the code for the most popular articles, but note that with this API we can paginate through results to get more than 20 reviews by querying the API multiple times. We retrieved a total of 53 movie reviews, 45 featuring Johnny Depp and 8 featuring Amber Heard.

```{r echo = FALSE}
extract_text <- function(json_result){
  reviews <- json_result$result %>% 
    dplyr::select(c('display_title','headline','link')) %>% 
    mutate(url = link$url) %>%
    select(-link)
  review_text <-tibble(headline = '',text='')
  for(i in 1:nrow(reviews)){
    url_html <- reviews[i,3]
    review_text <- review_text %>% add_row(headline = reviews[i,2], text = url_html %>% read_html() %>% html_nodes("section") %>% html_nodes("p") %>% html_text2())
  }
  review_text <- review_text %>% group_by(headline) %>% summarise(text = paste(text,collapse = " "))
  review_text <- review_text[-c(1),] 
  return(review_text)
}

url <- 'https://api.nytimes.com/svc/movies/v2/reviews/search.json?query=johnny+depp&api-key=52utY7TeqStbbVwadabbLam6qUdac7d5'
url20 <- 'https://api.nytimes.com/svc/movies/v2/reviews/search.json?query=johnny+depp&offset=20&api-key=52utY7TeqStbbVwadabbLam6qUdac7d5'
url40 <- 'https://api.nytimes.com/svc/movies/v2/reviews/search.json?query=johnny+depp&offset=40&api-key=52utY7TeqStbbVwadabbLam6qUdac7d5'
url_ah <- 'https://api.nytimes.com/svc/movies/v2/reviews/search.json?query=amber+heard&api-key=52utY7TeqStbbVwadabbLam6qUdac7d5'
url_ah20 <- 'https://api.nytimes.com/svc/movies/v2/reviews/search.json?query=amber+heard&offset=20&api-key=52utY7TeqStbbVwadabbLam6qUdac7d5'
json_result_jd <- url %>% curl() %>% readLines() %>% fromJSON()
json_result20 <- url20 %>% curl() %>% readLines() %>% fromJSON
json_result40 <- url40 %>% curl() %>% readLines() %>% fromJSON()
json_result_ah <- url_ah %>% curl() %>% readLines() %>% fromJSON()

review_text <- extract_text(json_result_jd)
review_text <- add_row(review_text, extract_text(json_result20))
review_text <- add_row(review_text, extract_text(json_result40))
review_text <- add_row(review_text, extract_text(json_result_ah))
review_text <- review_text %>% mutate(
  clean_text = tolower(text),
  clean_text = removeWords(clean_text,stop_words$word),
  clean_text = str_replace_all(clean_text,"[['`’$+\u0097]]", ""), 
  clean_text = str_replace_all(clean_text,"[[:punct:]]", " "),
  clean_text = str_replace_all(clean_text,'[[:digit:]]+', " "),
  clean_text = str_replace_all(clean_text,"[[:space:]]+", " "),
  clean_text = str_replace_all(clean_text," s ", " "),
  clean_text = trimws(clean_text)) 

dtm_reviews <- tm::DocumentTermMatrix(tm::VCorpus(tm::VectorSource(review_text$clean_text)),control=list(wordLengths=c(1,Inf)))
dtm_reviews
```

Let's jump straight into the LDA analysis. We were especially interested in seeing if the algorithm would differentiate between Amber Heard and Johnny Depp's movies. However, since the algorithm tends to keep the sizes of each topic at a relatively similar length and Johnny Depp has far more reviews than Amber Heard, we try separating the reviews into 6 topics.

```{r}
reviews_lda <- LDA(dtm_reviews, control = list(seed=0),k = 6, method = 'gibbs')
reviews_topics <- tidy(reviews_lda)
```

Next, we check the topics for the last 8 movies, which are the reviews for Amber Heard's movies.

```{r echo = FALSE}
reviews_lda %>% tidy(matrix="gamma") %>%
  group_by(document) %>%
  top_n(1) %>%
  mutate(document = as.numeric(document)) %>%
  arrange(desc(document)) %>%
  head(8)
```
We find that the topic modeling was unable to separate out Amber Heard's movies, so this experiment was a failure, but we still wanted to see if there were any notable topics. After some investigation we come across Topic 4. Here are the most probable words in this topic (asides from movie and film, which we filtered out because they were highly probable in every topic.).

```{r echo = FALSE}
reviews_topics %>% filter(topic==4) %>% 
  filter(term != 'movie', term != 'film') %>%
  mutate(beta_rank = min_rank(desc(beta))) %>% 
  filter(beta_rank <= 10) %>% 
  arrange(beta_rank) %>%
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(beta, term)) + 
  geom_col(show.legend = FALSE)
```

As you can see, this topic features words relating to arguably Johnny Depp's biggest franchise, the Pirates of the Caribbean. It is also one of two topics that does not contain any Amber Heard movie reviews.

We were not able to gain too many insights from LDA analysis, so we proceed to doing Sentiment Analysis to compare the sentiments of Johnny Depp's movie reviews versus Amber Heard's movie reviews. We begin by using the nrc sentiments.

First, Johnny Depp.

```{r echo = FALSE, message = FALSE}
sa_text_jd = review_text[1:45,] %>% group_by(headline) %>% unnest_tokens(word, clean_text)

sa_processed_text_jd = sa_text_jd %>% 
  inner_join(get_sentiments("nrc")) %>% group_by(headline) %>% count(sentiment, sort = TRUE)

sa_processed_text_jd_afinn = sa_text_jd %>% 
  inner_join(get_sentiments("afinn"))

sa_processed_text_jd %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ xlab("Sentiment") + 
  ylab("Number of Words") + 
  ggtitle("Sentiments in Reviews of Johnny Depp's Movies") + 
  theme(legend.position="none")

```

Then, Amber Heard.

```{r echo = FALSE, message = FALSE}
sa_text_ah = review_text[46:53,] %>% group_by(headline) %>% unnest_tokens(word, clean_text)

sa_processed_text_ah = sa_text_ah %>% 
  inner_join(get_sentiments("nrc")) %>% group_by(headline) %>% count(sentiment, sort = TRUE)

sa_processed_text_ah %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ xlab("Sentiment") + 
  ylab("Number of Words") + 
  ggtitle("Sentiments in Reviews of Amber Heard's Movies") + 
  theme(legend.position="none")
```

As you may notice, the trends seem to be quite similar for both Johnny Depp and Amber Heard, with Heard's movie reviews having a slightly higher proportion of negative and anger words. This may suggest that the reviews of Depp's movies are generally more positive. We can follow up by using the bing sentiments to look strictly at positive versus negative words.

```{r echo=FALSE, message=FALSE}
sa_processed_text_jd = sa_text_jd %>% 
  inner_join(get_sentiments("bing")) %>% group_by(headline) %>% count(sentiment, sort = TRUE)

jd_bing <- sa_processed_text_jd %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ xlab("Sentiment") + 
  ylab("Number of Words") + 
  ggtitle("Reviews of Johnny Depp's Movies") + 
  theme(legend.position="none")

sa_processed_text_ah = sa_text_ah %>% 
  inner_join(get_sentiments("bing")) %>% group_by(headline) %>% count(sentiment, sort = TRUE)

ah_bing <- sa_processed_text_ah %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ xlab("Sentiment") + 
  ylab("Number of Words") + 
  ggtitle("Reviews of Amber Heard's Movies") + 
  theme(legend.position="none")

gridExtra::grid.arrange(jd_bing, ah_bing, nrow = 1)
```

We can see here that even though the reviews for both of these two actors' movies are predominantly negative, the proportion of positive words is much higher for Johnny Depp's movies.

To investigate further, we use affinity, which compares the number of negative to positive words in a given review.

When we look at Johnny Depp, about 45% of the reviews of his movies are overall positive.

```{r echo = FALSE}
aggregate(x = sa_processed_text_jd_afinn$value, by = list(sa_processed_text_jd_afinn$headline),FUN = sum) %>% arrange(x) %>% ggplot(aes(seq(1:45), x, color=x)) +geom_bar(stat="identity") + xlab("Review Number") + ylab("Afinnity Score") + ggtitle("Afinnity of Reviews for Johnny Depp's Movies")
```

However when we look at Amber Heard, we see that only 1 of 8 of the reviews of her movies were overall positive, which is only 12.5%.

```{r echo = FALSE, message = FALSE}
sa_processed_text_ah_afinn = sa_text_ah %>% 
  inner_join(get_sentiments("afinn"))

aggregate(x = sa_processed_text_ah_afinn$value, by = list(sa_processed_text_ah_afinn$headline),FUN = sum) %>% arrange(x) %>% ggplot(aes(seq(1:8), x, color=x)) +geom_bar(stat="identity") + xlab("Review Number") + ylab("Afinnity Score") + ggtitle("Afinnity of Reviews for Amber Heard's Movies")
```

All these results in tandem seem to suggest that Johnny Depp's movies generally are much more positively received than Amber Heard's movies.

For the final portion of our project, we look into the personal lives and relationship between Johnny Depp and Amber Heard, as seen through the eyes of New York Times Articles written about them.

This is going to be a Temporal Sentiment Analysis of their relationship. 

We shall use the NYT's Article Search API for this purpose to pull up any articles on Johnny Depp and Amber Heard. Everything from when their realtionship in 2015 till now. 

Note: the results may have changed since the presentation

We start by using the code below:
```{r}
NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard+trial","&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
jd_ah_trial = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(jd_ah_trial)){
  jd_ah_trial$pub_date[index] = jd_ah_trial$pub_date[index] %>% substr(1,10)
}
jd_ah_trial$pub_date = as.Date(jd_ah_trial$pub_date)
jd_ah_trial = jd_ah_trial %>% arrange(pub_date)
jd_ah_trial$author = str_replace_all(jd_ah_trial$author,"By ", "")
```

But this is only giving us a few articles. The results are probably being limited, so we will specify time intervals and run this code repeatedly to get more data printed/published over the years. Below is the modified version.

```{r}
begin_date = 20150101
end_date = 20160101
NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
jd_ah_trial = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(jd_ah_trial)){
  jd_ah_trial$pub_date[index] = jd_ah_trial$pub_date[index] %>% substr(1,10)
}
jd_ah_trial$pub_date = as.Date(jd_ah_trial$pub_date)
jd_ah_trial = jd_ah_trial %>% arrange(pub_date)
jd_ah_trial$author = str_replace_all(jd_ah_trial$author,"By ", "")


begin_date = 20160101
end_date = 20170101
NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
new = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(new)){
  new$pub_date[index] = new$pub_date[index] %>% substr(1,10)
}
new$pub_date = as.Date(new$pub_date)
new = new %>% arrange(pub_date)
new$author = str_replace_all(new$author,"By ", "")

# We have to keep merging the data
jd_ah_trial = rbind(jd_ah_trial, new)

begin_date = 20170101
end_date = 20180101
NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
new = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(new)){
  new$pub_date[index] = new$pub_date[index] %>% substr(1,10)
}
new$pub_date = as.Date(new$pub_date)
new = new %>% arrange(pub_date)
new$author = str_replace_all(new$author,"By ", "")

# We have to keep merging the data
jd_ah_trial = rbind(jd_ah_trial, new)

begin_date = 20180101
end_date = 2020101
NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
new = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(new)){
  new$pub_date[index] = new$pub_date[index] %>% substr(1,10)
}
new$pub_date = as.Date(new$pub_date)
new = new %>% arrange(pub_date)
new$author = str_replace_all(new$author,"By ", "")

# We have to keep merging the data
jd_ah_trial = rbind(jd_ah_trial, new)

begin_date = 20200101
end_date = 20220505
NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
new = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(new)){
  new$pub_date[index] = new$pub_date[index] %>% substr(1,10)
}
new$pub_date = as.Date(new$pub_date)
new = new %>% arrange(pub_date)
new$author = str_replace_all(new$author,"By ", "")

# We have to keep merging the data
jd_ah_trial = rbind(jd_ah_trial, new)
```

Now that we have a good collection of 35 articles with search key johnny+depp+amber+heard since 2015, we move forward to our temporal analysis

First, extract text and clean it

```{r}
extract_text_article <- function(df){
  sa_text <-tibble(headline = '',text='')
  for(i in 1:nrow(df)){
    url_html <- df$url_html[i]
    sa_text <- sa_text %>% add_row(headline = df$headline[i], text = url_html %>% read_html() %>% html_nodes("section") %>% html_nodes("p") %>% html_text2())
  }
  sa_text <- sa_text %>% group_by(headline) %>% summarise(text = paste(text,collapse = " "))
  return(sa_text)
}
```

```{r}
sa_clean_text <- extract_text_article(jd_ah_trial)
sa_clean_text = sa_clean_text[-c(1),]
sa_clean_text <- sa_clean_text %>% mutate(
  clean_text = tolower(text),
  clean_text = removeWords(clean_text,stop_words$word),
  clean_text = str_replace_all(clean_text,"['`’”“]", ""), 
  clean_text = str_replace_all(clean_text,"[[:punct:]]", " "),
  clean_text = str_replace_all(clean_text,'[[:digit:]]+', " "),
  clean_text = str_replace_all(clean_text,"[[:space:]]+", " "),
  clean_text = trimws(clean_text)) 
```

Second, separating words for Sentiment Analysis

```{r}
sa_clean_text = sa_clean_text %>% group_by(headline) %>% unnest_tokens(word, clean_text)
```

Third, performing sentiment analysis and plotting results

```{r}
temporal_sentiments = sa_clean_text %>% inner_join(get_sentiments("nrc")) %>% group_by(headline) %>% count(sentiment)
temporal_sentiments
```

```{r}
temporal_sentiments %>% left_join(jd_ah_trial, by = "headline") %>% select(c(headline, sentiment, n, pub_date)) %>% arrange(pub_date) %>% mutate(prop = n/sum(n)) %>% ggplot(aes(pub_date, prop, color = sentiment)) + geom_line() + xlab('Time') + ylab('Magnitude of Sentiment') + ggtitle('Temporal NRC Sentiment Analysis: Johnny Depp v. Amber Heard')
```

We also perform Bing Sentiment Analysis

```{r}
temporal_sentiments = sa_clean_text %>% inner_join(get_sentiments("bing")) %>% group_by(headline) %>% count(sentiment)
temporal_sentiments
```

```{r}
temporal_sentiments %>% left_join(jd_ah_trial, by = "headline") %>% select(c(headline, sentiment, n, pub_date)) %>% arrange(pub_date) %>% ggplot(aes(pub_date, n, color = sentiment)) + geom_line() + xlab('Time') + ylab('Magnitude of Sentiment') + ggtitle('Temporal Bing Sentiment Analysis: Johnny Depp v. Amber Heard')
```

From the results obtained, we see that their relationship started out quite positive. And, that along with Joy were top sentiments for the most part which is until 2020. This is even after they got divorced; which makes sense since at the time, their separation was believed to be extremely respectful and mutual. There were no offensively outrageous reasons such as Domestic Violence or Abuse attached to it.

There wasn't much conversation after the private separation which is why we see the graphs sort of flattening between 2020 and 2022.

But, the story took started turning in 2022, when the lawsuits started, and the controversial op-eds came out. The overall sentiment portrayed is extremely negative, along with fear and anticipation on the rise. The world and the media is taken aback by the horrifying revelations during trial and have deeply condemned Amber Heard in their speech. Positive things have been spoken about Johnny Depp, but the magnitude of that sentiment is nothing as compared to the negativity. 

At this point, as the trial goes on, we expect to see more negativity towards the actors as the world sees past shiny curtain of Hollywood Celebrities and their complicated lives. 

Overall, our temporal sentiment analysis has agreed well with our understanding of the relationship that these two actors might have had and are having right now. We hope justice prevails through the trial, and would be interested in further using LDA topic modelling and Sentiment Analysis in future over this subject. 

Thank You!