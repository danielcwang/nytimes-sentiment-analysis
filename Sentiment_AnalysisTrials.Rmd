---
title: "Sentiment Analysis Trials"
author: "Pratiksha Ajit Sharma"
date: "24/04/2022"
output: html_document
---

```{r}
library(tidyverse)
library(httr)
library(curl)
library(jsonlite)
library(rvest)
library(stringr)
library(tidytext) 
library(textdata)
library(tm)
```

At first, we wanted to try some examples to get comfortable using the New York Times API with the help of their documentation to retrieve data and try doing some sample sentiment analysis on it. The following shows some sentiment analysis using the Movie Reviews API on reviews for movies that contain the word 'Godfather' in their name.

First, we import Godfather movie reviews

```{r}
url <- 'https://api.nytimes.com/svc/movies/v2/reviews/search.json?query=godfather&api-key=ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7'
json_result <- url %>% curl() %>% readLines() %>% fromJSON()
json_result
```

The above provided us metadata about the reviews, next we pick up URL for one review and try scraping the content of this review.

```{r}
url_html <- 'https://www.nytimes.com/2019/06/06/movies/the-black-godfather-review.html'
url_html %>% read_html() %>% html_nodes("section") %>% html_nodes("p") %>% html_text2()
```

Here, we create a function which extracts the text from each of the movie reviews using html web scraping

Please note, this function is specific to movie reviews at this point, since we only pick up some relevant columns from the data retrieved.

```{r}
extract_text <- function(json_result){
  reviews <- json_result$result %>% 
    dplyr::select(c('display_title','headline','link')) %>% 
    mutate(url = link$url) %>%
    select(-link)
  review_text <-tibble(headline = '',text='')
  for(i in 1:nrow(reviews)){
    url_html <- reviews[i,3]
    review_text <- review_text %>% add_row(headline = reviews[i,2], text = url_html %>% read_html() %>% html_nodes("section") %>% html_nodes("p") %>% html_text2())
  }
  review_text <- review_text %>% group_by(headline) %>% summarise(text = paste(text,collapse = " "))
  return(review_text)
}
```

Now, we clean the movie review text

```{r}
review_text <- extract_text(json_result)
review_text <- review_text %>% mutate(
  clean_text = tolower(text),
  clean_text = removeWords(clean_text,stop_words$word),
  clean_text = str_replace_all(clean_text,"['`]", ""), 
  clean_text = str_replace_all(clean_text,"[[:punct:]]", " "),
  clean_text = str_replace_all(clean_text,'[[:digit:]]+', " "),
  clean_text = str_replace_all(clean_text,"[[:space:]]+", " "),
  clean_text = trimws(clean_text)) 
```

And, now we are separating words for Sentiment Analysis

```{r}
tidy_text = review_text %>% group_by(headline) %>% unnest_tokens(word, clean_text)
```

Next, we started with NRC type sentiment analysis. So, we get all the sentiments partitioned in different dataframes

```{r}
nrc_joy <- get_sentiments("nrc") %>%  filter(sentiment == "joy")
nrc_positive <- get_sentiments("nrc") %>%  filter(sentiment == "positive")
nrc_negative <- get_sentiments("nrc") %>%  filter(sentiment == "negative")
nrc_anger <- get_sentiments("nrc") %>%  filter(sentiment == "anger")
nrc_anticipation <- get_sentiments("nrc") %>%  filter(sentiment == "anticipation")
nrc_disgust <- get_sentiments("nrc") %>%  filter(sentiment == "disgust")
nrc_fear <- get_sentiments("nrc") %>%  filter(sentiment == "fear")
nrc_surprise <- get_sentiments("nrc") %>%  filter(sentiment == "surprise")
nrc_sadness <- get_sentiments("nrc") %>%  filter(sentiment == "sadness")
nrc_trust <- get_sentiments("nrc") %>%  filter(sentiment == "trust")
```

Merging sentiments from reviews and our lexicons

```{r}
movie_reviews_joy = tidy_text %>% inner_join(nrc_joy) %>%count(word, sort = TRUE)
movie_reviews_positive = tidy_text %>% inner_join(nrc_positive) %>%count(word, sort = TRUE)
movie_reviews_negative = tidy_text %>% inner_join(nrc_negative) %>%count(word, sort = TRUE)
movie_reviews_anger = tidy_text %>% inner_join(nrc_anger) %>%count(word, sort = TRUE)
movie_reviews_anticipation = tidy_text %>% inner_join(nrc_anticipation) %>%count(word, sort = TRUE)
movie_reviews_disgust = tidy_text %>% inner_join(nrc_disgust) %>%count(word, sort = TRUE)
movie_reviews_fear = tidy_text %>% inner_join(nrc_fear) %>%count(word, sort = TRUE)
movie_reviews_surprise = tidy_text %>% inner_join(nrc_surprise) %>%count(word, sort = TRUE)
movie_reviews_sadness = tidy_text %>% inner_join(nrc_sadness) %>%count(word, sort = TRUE)
movie_reviews_trust = tidy_text %>% inner_join(nrc_trust) %>%count(word, sort = TRUE)
```

Now, we view one of our results

```{r}
movie_reviews_anger
```

And, it worked. Voila!

Next, we want to try with different types of sentiment analysis.

We go with Bing now following similar steps just like the NRC

```{r}
bing_positive <- get_sentiments("bing") %>%  filter(sentiment == "positive")
bing_negative <- get_sentiments("bing") %>%  filter(sentiment == "negative")
```


```{r}
movie_reviews_positive_bing = tidy_text %>% inner_join(bing_positive) %>%count(word, sort = TRUE)
movie_reviews_negative_bing = tidy_text %>% inner_join(bing_negative) %>%count(word, sort = TRUE)
```

Viewing the results

```{r}
head(movie_reviews_negative_bing)
```

```{r}
head(movie_reviews_positive_bing)
```

Next, afinn

```{r}
movie_reviews_afinn <- tidy_text %>% inner_join(get_sentiments("afinn"))
head(movie_reviews_afinn)
```

But we notice, that afinn does not make a lot of sense since the result does not portray an overall sentiment for a particular review. For NRC and Bing, you can filter results for a particular movie and the observations make sense, but there needs to be some aggregation required for afinn results to make sense.

```{r}
aggregate(x = movie_reviews_afinn$value, by = list(movie_reviews_afinn$headline),FUN = sum) %>% arrange(x)
```

The above assigns an overall score for a particular movie review which now helps us make observations.


With this, we have now experimented enough are comfortable using data from NYTimes API to perform Sentiment Analysis. So now, we start applying what we have learnt to topics specific to our project

We start by pulling the most popular articles from the NYTimes API and perform Sentiment Analysis on them. 

Most Popular API

Please note, appropriate changes have been made since we're now working with a different API that provides different data and metadata

```{r}
url2 <- 'https://api.nytimes.com/svc/mostpopular/v2/viewed/1.json?api-key=ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7'
json_result2 <- url2 %>% curl() %>% readLines() %>% fromJSON()
json_result2


extract_text_mp <- function(json_result){
  articles <- json_result$results %>% 
    dplyr::select(c('title','url'))
  article_text <-tibble(headline = '',text='')
  for(i in 1:nrow(articles)){
    url_html <- articles[i,2]
    article_text <- article_text %>% add_row(headline = articles[i,1], text = url_html %>% read_html() 
%>% html_nodes("section") %>% html_nodes("p") 
%>% html_text2())
  }
  article_text <- article_text %>% 
          group_by(headline) %>% 
summarise(text = paste(text,collapse = " "))
  article_text <- article_text [-c(1),] 
  return(article_text)
}

article_text <- extract_text_mp(json_result2)
article_text <- article_text %>% mutate(
  clean_text = tolower(text),
  clean_text = removeWords(clean_text,stop_words$word),
  clean_text = str_replace_all(clean_text,"['`]", ""), 
  clean_text = str_replace_all(clean_text,"[[:punct:]]", " "),
  clean_text = str_replace_all(clean_text,'[[:digit:]]+', " "),
  clean_text = str_replace_all(clean_text,"[[:space:]]+", " "),
  clean_text = trimws(clean_text))


sa_text = article_text %>% group_by(headline) %>% unnest_tokens(word, clean_text)

```

We now have the text from Most Popular Articles ready for analysis!

```{r}

sa_processed_text = sa_text %>% inner_join(get_sentiments("nrc")) %>% group_by(headline) %>% count(sentiment, sort = TRUE)

# Now, we plot this:

sa_processed_text %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Sentiment") + ylab("Number of Words") + ggtitle("Sentiments in Most Popular Articles") + theme(legend.position="none")

```

Next, we pick up a particular article and generate plots that show NRC and Bing analysis on it:

```{r}
sa_text %>% filter(headline == "Johnny Depp v. Amber Heard: What We Know") %>% inner_join(get_sentiments("nrc")) %>% count(sentiment, sort = TRUE) %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Sentiment") + ylab("Number of Words") + ggtitle("Johnny Depp v. Amber Heard: NRC SA") + theme(legend.position="none")

sa_text %>% filter(headline == "Johnny Depp v. Amber Heard: What We Know") %>% inner_join(get_sentiments("bing")) %>% count(sentiment, sort = TRUE) %>% ggplot(aes(reorder(sentiment,-n), n, color = sentiment)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Sentiment") + ylab("Number of Words") + ggtitle("Johnny Depp v. Amber Heard: Bing SA") + theme(legend.position="none")
```


We further use afinn to assign an affinity score to all articles that are most popular and plot them:

```{r}
sa_text_agg = sa_text %>% inner_join(get_sentiments("afinn"))  

aggregate(x = sa_text_agg$value, by = list(sa_text_agg$headline),FUN = sum) %>% arrange(x) %>% ggplot(aes(seq(1:20), x)) + geom_bar(stat = "identity") + xlab("Dummy Article Number") + ylab("Afinnity Score") + ggtitle("Afinnity of Most Popular Articles") + theme(legend.position="none")
```

Note: Article numbers are dummy


Now, Temporal Sentiment Analysis for Johnny Depp Amber Heard Relationship

We pull up articles from NYTimes' Article Search API

```{r}
NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard+trial","&facet_filter=true&api-key=",NYTIMES_KEY, sep="")

pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
jd_ah_trial = pages_new[1:6,] %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:6){
  jd_ah_trial$pub_date[index] = jd_ah_trial$pub_date[index] %>% substr(1,10)
}
jd_ah_trial$pub_date = as.Date(jd_ah_trial$pub_date)
jd_ah_trial = jd_ah_trial %>% arrange(pub_date)
jd_ah_trial$author = str_replace_all(jd_ah_trial$author,"By ", "")
```

This is however only giving us few articles. The results are probably being limited, so we will specify time intervals and run this code repeatedly to get more data printed/published over the years. Below is the modified version

```{r}
begin_date = 20150101
end_date = 20160101

NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")

pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
jd_ah_trial = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(jd_ah_trial)){
  jd_ah_trial$pub_date[index] = jd_ah_trial$pub_date[index] %>% substr(1,10)
}
jd_ah_trial$pub_date = as.Date(jd_ah_trial$pub_date)
jd_ah_trial = jd_ah_trial %>% arrange(pub_date)
jd_ah_trial$author = str_replace_all(jd_ah_trial$author,"By ", "")



begin_date = 20160101
end_date = 20170101

NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")

pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
new = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(new)){
  new$pub_date[index] = new$pub_date[index] %>% substr(1,10)
}
new$pub_date = as.Date(new$pub_date)
new = new %>% arrange(pub_date)
new$author = str_replace_all(new$author,"By ", "")


# We have to keep merging the data
jd_ah_trial = rbind(jd_ah_trial, new)


begin_date = 20170101
end_date = 20180101

NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")

pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
new = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(new)){
  new$pub_date[index] = new$pub_date[index] %>% substr(1,10)
}
new$pub_date = as.Date(new$pub_date)
new = new %>% arrange(pub_date)
new$author = str_replace_all(new$author,"By ", "")

# We have to keep merging the data
jd_ah_trial = rbind(jd_ah_trial, new)


begin_date = 20180101
end_date = 2020101

NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")

pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
new = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(new)){
  new$pub_date[index] = new$pub_date[index] %>% substr(1,10)
}
new$pub_date = as.Date(new$pub_date)
new = new %>% arrange(pub_date)
new$author = str_replace_all(new$author,"By ", "")

# We have to keep merging the data
jd_ah_trial = rbind(jd_ah_trial, new)


begin_date = 20200101
end_date = 20220505

NYTIMES_KEY = "ltnSuqlZoDGg6Td519vELgT8Fncs2Bq7"

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=","johnny+depp+amber+heard","&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")

pages_new = fromJSON(baseurl, flatten = TRUE) %>% data.frame() 
new = pages_new %>% select(abstract = response.docs.abstract, url_html = response.docs.web_url, lead_para = response.docs.lead_paragraph, pub_date = response.docs.pub_date, headline = response.docs.headline.main, author = response.docs.byline.original)
for (index in 1:nrow(new)){
  new$pub_date[index] = new$pub_date[index] %>% substr(1,10)
}
new$pub_date = as.Date(new$pub_date)
new = new %>% arrange(pub_date)
new$author = str_replace_all(new$author,"By ", "")

# We have to keep merging the data
jd_ah_trial = rbind(jd_ah_trial, new)
```

Now, extract text, clean and perform temporal sentiment analysis

```{r}
extract_text_article <- function(df){
  sa_text <-tibble(headline = '',text='')
  for(i in 1:nrow(df)){
    url_html <- df$url_html[i]
    sa_text <- sa_text %>% add_row(headline = df$headline[i], text = url_html %>% read_html() %>% html_nodes("section") %>% html_nodes("p") %>% html_text2())
  }
  sa_text <- sa_text %>% group_by(headline) %>% summarise(text = paste(text,collapse = " "))
  return(sa_text)
}
```

```{r}
sa_clean_text <- extract_text_article(jd_ah_trial)
sa_clean_text = sa_clean_text[-c(1),]
sa_clean_text <- sa_clean_text %>% mutate(
  clean_text = tolower(text),
  clean_text = removeWords(clean_text,stop_words$word),
  clean_text = str_replace_all(clean_text,"['`’”“]", ""), 
  clean_text = str_replace_all(clean_text,"[[:punct:]]", " "),
  clean_text = str_replace_all(clean_text,'[[:digit:]]+', " "),
  clean_text = str_replace_all(clean_text,"[[:space:]]+", " "),
  clean_text = trimws(clean_text)) 
```

Separating words for Sentiment Analysis

```{r}
sa_clean_text = sa_clean_text %>% group_by(headline) %>% unnest_tokens(word, clean_text)
```

```{r}
temporal_sentiments = sa_clean_text %>% inner_join(get_sentiments("nrc")) %>% group_by(headline) %>% count(sentiment)
temporal_sentiments
```

```{r}
temporal_sentiments %>% left_join(jd_ah_trial, by = "headline") %>% select(c(headline, sentiment, n, pub_date)) %>% arrange(pub_date) %>% mutate(prop = n/sum(n)) %>% ggplot(aes(pub_date, prop, color = sentiment)) + geom_line() + xlab('Time') + ylab('Magnitude of Sentiment') + ggtitle('Temporal NRC Sentiment Analysis: Johnny Depp v. Amber Heard')
```


```{r}
temporal_sentiments = sa_clean_text %>% inner_join(get_sentiments("bing")) %>% group_by(headline) %>% count(sentiment)
temporal_sentiments
```

```{r}
temporal_sentiments %>% left_join(jd_ah_trial, by = "headline") %>% select(c(headline, sentiment, n, pub_date)) %>% arrange(pub_date) %>% ggplot(aes(pub_date, n, color = sentiment)) + geom_line() + xlab('Time') + ylab('Magnitude of Sentiment') + ggtitle('Temporal Bing Sentiment Analysis: Johnny Depp v. Amber Heard')
```
